{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josedossantos10/HIRS/blob/main/Hibrido_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajuste os parâmentros baixo e depois execute todo o codigo para avaliar o modelo hibrido"
      ],
      "metadata": {
        "id": "HW9fjpqQG_dJ"
      },
      "id": "HW9fjpqQG_dJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de documentos a serem recuperados\n",
        "top_k = 20\n",
        "# Coluna a ser usada para avaliação na camada lexica\n",
        "column_data_bm25 = 'txtInteiroTeor'\n",
        "# Coluna a ser usada para avaliação na camada semântica\n",
        "column_data_model = 'txtInteiroTeor'\n",
        "# Caminho até o modelos semaântico (com Fine-Tuning) que será usado para recuperar os documentos\n",
        "version = '/content/drive/MyDrive/Sentence Model Tuned/txtInteiroTeor_sbert'"
      ],
      "metadata": {
        "id": "VyQT286uHSZH"
      },
      "id": "VyQT286uHSZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ntmAWeAXIdqE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntmAWeAXIdqE",
        "outputId": "8ee9ea7b-8200-4f4e-dd76-0063d65468bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MYa48eRGvaiM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYa48eRGvaiM",
        "outputId": "c6c0bfcf-8379-4bc7-aac5-298a9e2fae8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IyQy2-iH7QdU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyQy2-iH7QdU",
        "outputId": "62df1f58-8540-48f3-d8ba-f7a72aea14d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ],
      "source": [
        "#Importando a biblioteca nltk\n",
        "\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, SentencesDataset, util, SentencesDataset, InputExample, losses\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VG7jT7SWrvhU",
      "metadata": {
        "id": "VG7jT7SWrvhU"
      },
      "source": [
        "## BM25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zj36SyxgdTtJ",
      "metadata": {
        "id": "zj36SyxgdTtJ"
      },
      "source": [
        " ### *BM25* Okapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "random-charleston",
      "metadata": {
        "id": "random-charleston"
      },
      "outputs": [],
      "source": [
        "#Algoritmo do BM25 Okapi\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "\"\"\"\n",
        "All of these algorithms have been taken from the paper:\n",
        "Trotmam et al, Improvements to BM25 and Language Models Examined\n",
        "Here we implement all the BM25 variations mentioned.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BM25:\n",
        "    def __init__(self, corpus, tokenizer=None):\n",
        "        self.corpus_size = len(corpus)\n",
        "        self.avgdl = 0\n",
        "        self.doc_freqs = []\n",
        "        self.idf = {}\n",
        "        self.doc_len = []\n",
        "        self.tokenizer = tokenizer\n",
        "        #Tokenizar o corpus\n",
        "        if tokenizer:\n",
        "            corpus = self._tokenize_corpus(corpus)\n",
        "\n",
        "        nd = self._initialize(corpus)\n",
        "        self._calc_idf(nd)\n",
        "\n",
        "    def _initialize(self, corpus):\n",
        "        nd = {}  # word -> number of documents with word\n",
        "        num_doc = 0\n",
        "        for document in corpus:\n",
        "            self.doc_len.append(len(document))\n",
        "            num_doc += len(document)\n",
        "\n",
        "            frequencies = {}\n",
        "            for word in document:\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 0\n",
        "                frequencies[word] += 1\n",
        "            self.doc_freqs.append(frequencies)\n",
        "\n",
        "            for word, freq in frequencies.items():\n",
        "                try:\n",
        "                    nd[word] += 1\n",
        "                except KeyError:\n",
        "                    nd[word] = 1\n",
        "\n",
        "        self.avgdl = num_doc / self.corpus_size\n",
        "\n",
        "        return nd\n",
        "\n",
        "    def _tokenize_corpus(self, corpus):\n",
        "        pool = Pool(cpu_count())\n",
        "        tokenized_corpus = pool.map(self.tokenizer, corpus)\n",
        "        return tokenized_corpus\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_top_n(self, query, documents, n=5):\n",
        "\n",
        "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
        "\n",
        "        scores = self.get_scores(query)\n",
        "        top_n = np.argsort(scores)[::-1][:n]\n",
        "        return [documents[i] for i in top_n]\n",
        "\n",
        "    def get_top_n_ngram(self, score, documents, n=5):\n",
        "\n",
        "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
        "\n",
        "        pdr = np.zeros(self.corpus_size)\n",
        "        alpha = 2\n",
        "        for ps in score:\n",
        "            pdr = np.add(pdr, alpha * ps)\n",
        "            alpha -= 1\n",
        "\n",
        "        top_n = np.argsort(pdr)[::-1][:n]\n",
        "        return [documents[i] for i in top_n]\n",
        "\n",
        "    def get_partial_score(self, query, documents):\n",
        "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
        "\n",
        "        score = self.get_scores(query)\n",
        "\n",
        "        return score\n",
        "\n",
        "#BM25Okapi\n",
        "class BM25Okapi(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    #Calculo do IDF (Inverse Document Frequency)\n",
        "    def _calc_idf(self, nd):\n",
        "        \"\"\"\n",
        "        Calculates frequencies of terms in documents and in corpus.\n",
        "        This algorithm sets a floor on the idf values to eps * average_idf\n",
        "        \"\"\"\n",
        "        # collect idf sum to calculate an average idf for epsilon value\n",
        "        idf_sum = 0\n",
        "        # collect words with negative idf to set them a special epsilon value.\n",
        "        # idf can be negative if word is contained in more than half of documents\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    #Avaliar a pontuacao de todos os documentos na base\n",
        "    def get_scores(self, query):\n",
        "        \"\"\"\n",
        "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
        "        this algorithm also adds a floor to the idf value of epsilon.\n",
        "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
        "        :param query:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        \"\"\"\n",
        "        Calculate bm25 scores between query and subset of all docs\n",
        "        \"\"\"\n",
        "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
        "        score = np.zeros(len(doc_ids))\n",
        "        doc_len = np.array(self.doc_len)[doc_ids]\n",
        "        for q in query:\n",
        "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Az0F-1IVdRf8",
      "metadata": {
        "id": "Az0F-1IVdRf8"
      },
      "source": [
        "### BM25L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5DJ05SG-o7Q",
      "metadata": {
        "id": "d5DJ05SG-o7Q"
      },
      "outputs": [],
      "source": [
        "#Implementacao do BM25L - adapta parametros para corrigir a preferencia do Okapi por documentos mais curtos\n",
        "\n",
        "class BM25L(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    #Calculo do IDF (Inverse Document Frequency)\n",
        "    def _calc_idf(self, nd):\n",
        "        \"\"\"\n",
        "        Calculates frequencies of terms in documents and in corpus.\n",
        "        This algorithm sets a floor on the idf values to eps * average_idf\n",
        "        \"\"\"\n",
        "        # collect idf sum to calculate an average idf for epsilon value\n",
        "        idf_sum = 0\n",
        "        # collect words with negative idf to set them a special epsilon value.\n",
        "        # idf can be negative if word is contained in more than half of documents\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size + 1) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    #Calculo do ctd\n",
        "    def get_ctd(self, q_freq, b, doc_len, avg_len):\n",
        "      ctd = q_freq/(1 - b + b*(doc_len)/(avg_len))\n",
        "      return ctd\n",
        "\n",
        "    #Avaliar a pontuacao de todos os documentos na base\n",
        "    def get_scores(self, query):\n",
        "        \"\"\"\n",
        "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
        "        this algorithm also adds a floor to the idf value of epsilon.\n",
        "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
        "        :param query:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            ctd = self.get_ctd(q_freq, self.b, doc_len, self.avgdl)\n",
        "            score += (self.idf.get(q) or 0) * ( (ctd + 0.5) * (self.k1 + 1) /\n",
        "                                               ( (ctd + 0.5) + self.k1 ))\n",
        "        return score\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        \"\"\"\n",
        "        Calculate bm25 scores between query and subset of all docs\n",
        "        \"\"\"\n",
        "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
        "        score = np.zeros(len(doc_ids))\n",
        "        doc_len = np.array(self.doc_len)[doc_ids]\n",
        "        for q in query:\n",
        "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dyq44KaJyjTu",
      "metadata": {
        "id": "Dyq44KaJyjTu"
      },
      "source": [
        "### BM25Plus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wypOu5iSyjTv",
      "metadata": {
        "id": "wypOu5iSyjTv"
      },
      "outputs": [],
      "source": [
        "#BM25+\n",
        "class BM25plus(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    #Calculo do IDF (Inverse Document Frequency)\n",
        "    def _calc_idf(self, nd):\n",
        "        \"\"\"\n",
        "        Calculates frequencies of terms in documents and in corpus.\n",
        "        This algorithm sets a floor on the idf values to eps * average_idf\n",
        "        \"\"\"\n",
        "        # collect idf sum to calculate an average idf for epsilon value\n",
        "        idf_sum = 0\n",
        "        # collect words with negative idf to set them a special epsilon value.\n",
        "        # idf can be negative if word is contained in more than half of documents\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size + 1) - math.log(freq)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    #Avaliar pontuação de todos os documentos na base\n",
        "    def get_scores(self, query):\n",
        "        \"\"\"\n",
        "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
        "        this algorithm also adds a floor to the idf value of epsilon.\n",
        "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
        "        :param query:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * ( (q_freq * (self.k1 + 1)) /\n",
        "                                               (self.k1*((1-self.b) + self.b*((doc_len)/(self.avgdl)) + q_freq) + 1))\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uW6YHuySdaRK",
      "metadata": {
        "id": "uW6YHuySdaRK"
      },
      "source": [
        "## Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gUIdaC6prgcU",
      "metadata": {
        "id": "gUIdaC6prgcU"
      },
      "source": [
        "### Savoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dIVIVepprhf_",
      "metadata": {
        "id": "dIVIVepprhf_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Savoy:\n",
        "\n",
        "    def __removeAllPTAccent(self, old_word):\n",
        "        word = list(old_word)\n",
        "        len_word = len(word)-1\n",
        "        for i in range(len_word, -1, -1):\n",
        "            if word[i] == 'ä':\n",
        "                word[i] = 'a'\n",
        "            if word[i] == 'â':\n",
        "                word[i] = 'a'\n",
        "            if word[i] == 'à':\n",
        "                word[i] = 'a'\n",
        "            if word[i] == 'á':\n",
        "                word[i] = 'a'\n",
        "            if word[i] == 'ã':\n",
        "                word[i] = 'a'\n",
        "            if word[i] == 'ê':\n",
        "                word[i] = 'e'\n",
        "            if word[i] == 'é':\n",
        "                word[i] = 'e'\n",
        "            if word[i] == 'è':\n",
        "                word[i] = 'e'\n",
        "            if word[i] == 'ë':\n",
        "                word[i] = 'e'\n",
        "            if word[i] == 'ï':\n",
        "                word[i] = 'i'\n",
        "            if word[i] == 'î':\n",
        "                word[i] = 'i'\n",
        "            if word[i] == 'ì':\n",
        "                word[i] = 'i'\n",
        "            if word[i] == 'í':\n",
        "                word[i] = 'i'\n",
        "            if word[i] == 'ü':\n",
        "                word[i] = 'u'\n",
        "            if word[i] == 'ú':\n",
        "                word[i] = 'u'\n",
        "            if word[i] == 'ù':\n",
        "                word[i] = 'u'\n",
        "            if word[i] == 'û':\n",
        "                word[i] = 'u'\n",
        "            if word[i] == 'ô':\n",
        "                word[i] = 'o'\n",
        "            if word[i] == 'ö':\n",
        "                word[i] = 'o'\n",
        "            if word[i] == 'ó':\n",
        "                word[i] = 'o'\n",
        "            if word[i] == 'ò':\n",
        "                word[i] = 'o'\n",
        "            if word[i] == 'õ':\n",
        "                word[i] = 'o'\n",
        "            if word[i] == 'ç':\n",
        "                word[i] = 'c'\n",
        "\n",
        "        new_word = \"\".join(word)\n",
        "        return new_word\n",
        "\n",
        "    def __finalVowelPortuguese(self, word):\n",
        "        len_word = len(word)\n",
        "        if len_word > 3:\n",
        "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
        "                word = word[:-1]\n",
        "\n",
        "        return word\n",
        "\n",
        "    def __remove_PTsuffix(self, word):\n",
        "        len_word = len(word)\n",
        "\n",
        "        if len_word > 3:\n",
        "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
        "                word = word[:-2]\n",
        "                return word\n",
        "        if len_word > 2:\n",
        "            if word[-1] == 's' and word[-2] == 'n':\n",
        "                new_word = list(word)\n",
        "                new_word[-2] = 'm'\n",
        "                sing = \"\".join(new_word)\n",
        "                sing = sing[:-1]\n",
        "                return sing\n",
        "\n",
        "        if len_word > 3:\n",
        "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
        "                new_word = list(word)\n",
        "                new_word[-3] = 'e'\n",
        "                new_word[-2] = 'l'\n",
        "                sing = \"\".join(new_word)\n",
        "                sing = sing[:-1]\n",
        "                return sing\n",
        "\n",
        "        if len_word > 3:\n",
        "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
        "                new_word = list(word)\n",
        "                new_word[-2] = 'l'\n",
        "                sing = \"\".join(new_word)\n",
        "                sing = sing[:-1]\n",
        "                return sing\n",
        "\n",
        "        if len_word > 3:\n",
        "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
        "                new_word = list(word)\n",
        "                new_word[-3] = 'o'\n",
        "                new_word[-2] = 'l'\n",
        "                sing = \"\".join(new_word)\n",
        "                sing = sing[:-1]\n",
        "                return sing\n",
        "\n",
        "        if len_word > 3:\n",
        "            if word[-1] == 's' and word[-2] == 'i':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'l'\n",
        "                sing = \"\".join(new_word)\n",
        "                return sing\n",
        "\n",
        "        if len_word > 2:\n",
        "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
        "                new_word = list(word)\n",
        "                new_word[-3] = 'ã'\n",
        "                new_word[-2] = 'o'\n",
        "                sing = \"\".join(new_word)\n",
        "                sing = sing[:-1]\n",
        "                return sing\n",
        "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
        "                new_word = list(word)\n",
        "                new_word[-2] = 'o'\n",
        "                sing = \"\".join(new_word)\n",
        "                sing = sing[:-1]\n",
        "                return sing\n",
        "\n",
        "        if len_word > 5:\n",
        "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
        "                word = word[:-5]\n",
        "                return word\n",
        "\n",
        "        if len_word > 2:\n",
        "            if word[-1] == 's':\n",
        "                word = word[:-1]\n",
        "\n",
        "        return word\n",
        "\n",
        "    def __normFemininPortuguese(self, word):\n",
        "\n",
        "        len_word = len(word)\n",
        "\n",
        "        if len_word < 3 or word[-1] != 'a':\n",
        "            return word\n",
        "\n",
        "        if len_word > 6:\n",
        "\n",
        "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "        if len_word > 5:\n",
        "            if word[-2] == 'n' and word[-3] == 'o':\n",
        "                new_word = list(word)\n",
        "                new_word[-3] = 'ã'\n",
        "                new_word[-2] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                masc = masc[:-1]\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'r' and word[-3] == 'o':\n",
        "                word = word[:-1]\n",
        "                return word\n",
        "\n",
        "            if word[-2] == 's' and word[-3] == 'o':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 's' and word[-3] == 'e':\n",
        "                new_word = list(word)\n",
        "                new_word[-3] = 'ê'\n",
        "                masc = \"\".join(new_word)\n",
        "                masc = masc[:-1]\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'c' and word[-3] == 'i':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'd' and word[-3] == 'i':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'd' and word[-3] == 'a':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'v' and word[-3] == 'i':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'm' and word[-3] == 'a':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "            if word[-2] == 'n':\n",
        "                new_word = list(word)\n",
        "                new_word[-1] = 'o'\n",
        "                masc = \"\".join(new_word)\n",
        "                return masc\n",
        "\n",
        "        return word\n",
        "\n",
        "    def stem(self, word):\n",
        "        len_word = len(word)\n",
        "        if len_word > 2:\n",
        "            word = self.__remove_PTsuffix(word)\n",
        "            word = self.__normFemininPortuguese(word)\n",
        "            word = self.__finalVowelPortuguese(word)\n",
        "            word = self.__removeAllPTAccent(word)\n",
        "\n",
        "        return word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p4XhMlK6rcuM",
      "metadata": {
        "id": "p4XhMlK6rcuM"
      },
      "source": [
        "### Aplicando pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optical-winner",
      "metadata": {
        "id": "optical-winner"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from unicodedata import normalize\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Remove os acentos de uma string\n",
        "def _remove_acentos(txt):\n",
        "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "# Tokenizador\n",
        "def preprocess0(txt):\n",
        "    terms = word_tokenize(txt)\n",
        "    # tokenizer = RegexpTokenizer('\\w+')\n",
        "    # terms = tokenizer.tokenize(txt.lower())\n",
        " #   terms = [word for word in terms if word not in stopwords]\n",
        "\n",
        "    return terms\n",
        "\n",
        "# Remoção de stopwords + acentuação\n",
        "def preprocess2(txt):\n",
        "    txt = _remove_acentos(txt)\n",
        "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "    stopwords.extend(list(punctuation))\n",
        "\n",
        "    # terms = word_tokenize(txt.lower())\n",
        "    tokenizer = RegexpTokenizer('\\w+')\n",
        "    terms = tokenizer.tokenize(txt.lower())\n",
        "    terms = [word for word in terms if word not in stopwords]\n",
        "\n",
        "    return terms\n",
        "\n",
        "# Remoção de stopwords + acentuação + steming\n",
        "def preprocess3(txt):\n",
        "    txt = _remove_acentos(txt)\n",
        "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "    stopwords.extend(list(punctuation))\n",
        "\n",
        "    # stemmer = RSLPStemmer()\n",
        "    # terms = word_tokenize(txt.lower())\n",
        "    stemmer = Savoy()\n",
        "    tokenizer = RegexpTokenizer('\\w+')\n",
        "    terms = tokenizer.tokenize(txt.lower())\n",
        "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
        "    return terms\n",
        "\n",
        "# Remoção de stopwords + acentuação + n-gram\n",
        "def preprocess_ngram(txt, n):\n",
        "    txt = _remove_acentos(txt)\n",
        "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "    stopwords.extend(list(punctuation))\n",
        "\n",
        "    # terms = word_tokenize(txt.lower())\n",
        "    tokenizer = RegexpTokenizer('\\w+')\n",
        "    terms = tokenizer.tokenize(txt.lower())\n",
        "    terms = [word for word in terms if word not in stopwords]\n",
        "\n",
        "    ngram = list(ngrams(terms, n))\n",
        "\n",
        "    return ngram\n",
        "\n",
        "# Remoção de stopwords + acentuação + steming + n-gram\n",
        "def preprocess_ngram_stem(txt, n):\n",
        "    txt = _remove_acentos(txt)\n",
        "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "    stopwords.extend(list(punctuation))\n",
        "\n",
        "    # stemmer = RSLPStemmer()\n",
        "    # terms = word_tokenize(txt.lower())\n",
        "    stemmer = Savoy()\n",
        "    tokenizer = RegexpTokenizer('\\w+')\n",
        "    terms = tokenizer.tokenize(txt.lower())\n",
        "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
        "\n",
        "    ngram = list(ngrams(terms, n))\n",
        "\n",
        "    return ngram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thermal-incidence",
      "metadata": {
        "id": "thermal-incidence"
      },
      "outputs": [],
      "source": [
        "# Obtem o nome do texto no df\n",
        "def get_name(df, doc,column_data):\n",
        "    return str(df[df[column_data]==doc].txtNome.to_numpy()[0]).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a61b26",
      "metadata": {
        "id": "30a61b26"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(corpus_embeddings, query_embedding):\n",
        "    # corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "    res_vector = list()\n",
        "    try:\n",
        "        # query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "        # top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "        # if torch.cuda.is_available():\n",
        "        #     top_resuts_txt = [get_name(df, txt, in_field=column_data, out_field=\"txtNome\").strip() for txt in corpus[top_results[1].cpu().numpy()]]\n",
        "        # else:\n",
        "        #     top_resuts_txt = [get_name(df, txt, in_field=column_data, out_field=\"txtNome\").strip() for txt in corpus[top_results[1].numpy()]]\n",
        "\n",
        "        # res_vector.append([get_name(df_assunto, query, \"TxtAssunto\", \"NÚMERO-PROPOSIÇÃOSILEG\").strip(), top_resuts_txt])\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao realizar encoding:{e}\")\n",
        "        pass\n",
        "    if torch.cuda.is_available():\n",
        "      return cos_scores.cpu().numpy()\n",
        "    return cos_scores.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie uma pasta data usando o pytohn\n",
        "\n",
        "import os\n",
        "if not os.path.exists('data'):\n",
        "  os.makedirs('data')\n"
      ],
      "metadata": {
        "id": "S4Y1XszAa1_U"
      },
      "id": "S4Y1XszAa1_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qmy611rGwQX9",
      "metadata": {
        "id": "Qmy611rGwQX9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "def get_file(url, folder='data'):\n",
        "  r = requests.get(url.split('?').pop(0)+'?download=1')\n",
        "  d = r.headers['content-disposition']\n",
        "  fname = re.findall(\"filename=(.+)\", d)[0].replace('\"','')\n",
        "  with open(f'/content/{folder}/{fname}','wb') as f:\n",
        "    f.write(r.content)\n",
        "  if '.zip' in fname:\n",
        "    !unzip -o '/content/{folder}/{fname}' -d '/content/{folder}/'\n",
        "    print('Unziped and',end=' ')\n",
        "  print('Saved '+fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nabw7ULgwMBD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nabw7ULgwMBD",
        "outputId": "3d905333-48d1-4218-a6a8-4605a6724796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/base_20230428_douglas-bill_corpus-parts.zip\n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part0.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part1.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part2.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part3.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part4.csv  \n",
            "Unziped and Saved base_20230428_douglas-bill_corpus-parts.zip\n",
            "Saved pares-arvores-proposicao-filter.csv\n",
            "Archive:  /content/data/dados-conle-anonimizado-job_request-parts.zip\n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part0.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part1.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part2.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part3.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part4.csv  \n",
            "Unziped and Saved dados-conle-anonimizado-job_request-parts.zip\n"
          ]
        }
      ],
      "source": [
        "get_file('https://ufrpebr-my.sharepoint.com/:u:/g/personal/joseantonio_santos_ufrpe_br/EQcLlSaOKPtCtzY3juKPkpYB6Rl9Jnp-xNimWaIcJNe_bg?e=jc0ZWR')\n",
        "get_file('https://ufrpebr-my.sharepoint.com/:x:/g/personal/joseantonio_santos_ufrpe_br/EfciCkFDXkxAoKnvwW-O0FYB1YMf-iXIwD1nVdf0Ve_t8g?e=X1QPUi')\n",
        "get_file('https://ufrpebr-my.sharepoint.com/:u:/g/personal/joseantonio_santos_ufrpe_br/ESigtraTg-xLjUAwHuEzF0sB-CkOhU75tNSZcomdbuvpxg?e=AcMaXi')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MQVFFLJav0i3",
      "metadata": {
        "id": "MQVFFLJav0i3"
      },
      "source": [
        "# Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(df, arr_assunto, top_k, all_preprocessing, column_data_bm25,column_data_model, model_name):\n",
        "    data = df[column_data_bm25].to_numpy()\n",
        "    if all_preprocessing:\n",
        "        tokenized_corpus3 = [preprocess3(doc) for doc in data]                      #Stopwords + Acentuação + Stemming\n",
        "        bigram_corpus_stem = [preprocess_ngram_stem(doc, 2) for doc in data]        #Stopwords + Acentuação + Stemming + Bigram\n",
        "        bm25L_bigram_stem = BM25L(bigram_corpus_stem)           #Stopwords + Acentuação + Stemming + Bigram             #Tokenizer\n",
        "    else:\n",
        "        tokenized_corpus3 = [preprocess0(doc) for doc in data]\n",
        "    bm25L_tokenstem = BM25L(tokenized_corpus3)              #Stopwords + Acentuação + Stemming\n",
        "\n",
        "    y,X = arr_assunto[:,0],arr_assunto[:,1]\n",
        "    y = [i.strip() for i in y]\n",
        "    model = SentenceTransformer(model_name)\n",
        "    model.max_seq_length=512\n",
        "\n",
        "    corpus_embeddings = model.encode(df[column_data_model].to_numpy(), convert_to_tensor=True)\n",
        "\n",
        "    '''\n",
        "    Obter o resultado do BM25 para cada query\n",
        "    '''\n",
        "    #Listas com os n documentos mais relevantes para cada versao do BM25\n",
        "    labels_nsL = list()             #Stopwords + Acentuação + Stemming + Unigram + Bigram (L)\n",
        "    l_v = list()\n",
        "    for l,x in zip((y),X):\n",
        "        if all_preprocessing:\n",
        "            tokenized_query3 = preprocess3(x)                   #Stopwords + Acentuação + Stemming\n",
        "        else:\n",
        "            tokenized_query3 = preprocess0(x)                   #Tokenizer\n",
        "        query_embedding = model.encode(x, convert_to_tensor=True)\n",
        "        scores_stem_l = list()\n",
        "        scores = bm25L_tokenstem.get_partial_score(tokenized_query3, data)\n",
        "        scores_normalized = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))\n",
        "\n",
        "\n",
        "        model_score = evaluate_model(query_embedding=query_embedding, corpus_embeddings=corpus_embeddings)\n",
        "        model_score_normalized = (model_score - np.min(model_score)) / (np.max(model_score) - np.min(model_score))\n",
        "        if all_preprocessing:\n",
        "            # final_score = scores_normalized + scores_bi_normalized + model_score_normalized\n",
        "            final_score = scores_normalized + model_score_normalized\n",
        "        else:\n",
        "            final_score = scores_normalized + model_score_normalized\n",
        "        scores_stem_l.append(final_score)\n",
        "        top_n_stem_l = bm25L_tokenstem.get_top_n_ngram(scores_stem_l, data, n=top_k)\n",
        "\n",
        "        labelsnsL = [get_name(df,d,column_data_bm25) for d in top_n_stem_l]              #L\n",
        "        #Adicionar os resultados da query as listas\n",
        "        labels_nsL.append(labelsnsL)    #L\n",
        "\n",
        "        #Adicionar resposta esperada a lista\n",
        "        l_v.append(l)\n",
        "\n",
        "    res_nsL = list()                #Stopwords + Acentuação + Stemming + Unigram + Bigram (L)\n",
        "    for k in range(1, top_k+1):\n",
        "        resultsnsL = list()         #Stopwords + Acentuação + Stemming + Unigram + Bigram (L)\n",
        "        for i in range(len(l_v)):\n",
        "            if (l_v[i] in labels_nsL[i][:k]):\n",
        "                resultsnsL.append(1)\n",
        "            else:\n",
        "                resultsnsL.append(0)\n",
        "        res_nsL.append(sum(resultsnsL)/len(resultsnsL))\n",
        "\n",
        "    return res_nsL[-1]"
      ],
      "metadata": {
        "id": "iE3tz5sFXOr2"
      },
      "id": "iE3tz5sFXOr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Verificar se a solicitação correta esta entre os k primeiro documentos retornados (de 1 ate k=20)\n",
        "'''\n",
        "# top_k = 20\n",
        "# column_data_bm25 = 'txtInteiroTeorLimpo'\n",
        "# column_data_model = 'txtInteiroTeorLimpo'\n",
        "# version = '/content/drive/MyDrive/Sentence Model Tuned/txtInteiroTeor_sbert'\n",
        "runs = 5\n",
        "recalls_processed = []\n",
        "\n",
        "for i in tqdm(range(0, runs)):\n",
        "    element = f'{column_data_bm25}_{i}'\n",
        "    df = pd.read_csv(f\"/content/data/base_20230428_douglas-bill_corpus-part{i}.csv\")\n",
        "    df_assunto = pd.read_csv(f\"/content/data/dados-conle-anonimizado-job_request-part{i}.csv\", encoding=\"utf-8\").to_numpy()\n",
        "    recalls_processed.append(evaluate(df,df_assunto,top_k, True,column_data_bm25,column_data_model,version))\n",
        "print(f'BM25L preprocessed {recalls_processed} --> Média: {round(np.average(recalls_processed)*100,2)}. Desvio Padrão: {np.std(recalls_processed)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOW7K9UbX1yw",
        "outputId": "274926f9-e3ea-4ef9-cf01-45458920adc1"
      },
      "id": "LOW7K9UbX1yw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [2:01:46<00:00, 1461.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25L preprocessed [0.6888888888888889, 0.8648648648648649, 0.8656716417910447, 0.9622641509433962, 0.8571428571428571] --> Média: 84.78. Desvio Padrão: 0.08837722742970304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Verificar se a solicitação correta esta entre os k primeiro documentos retornados (de 1 ate 100)\n",
        "'''\n",
        "top_k = 20\n",
        "column_data_bm25 = 'txtInteiroTeor'\n",
        "column_data_model = 'txtInteiroTeor'\n",
        "version = '/content/drive/MyDrive/Sentence Model Tuned/txtInteiroTeor_sbert'\n",
        "runs = 5\n",
        "recalls_processed = []\n",
        "\n",
        "for i in tqdm(range(0, runs)):\n",
        "    element = f'{column_data_bm25}_{i}'\n",
        "    df = pd.read_csv(f\"/content/data/base_20230428_douglas-bill_corpus-part{i}.csv\")\n",
        "    df_assunto = pd.read_csv(f\"/content/data/dados-conle-anonimizado-job_request-part{i}.csv\", encoding=\"utf-8\").to_numpy()\n",
        "    recalls_processed.append(evaluate(df,df_assunto,top_k, True,column_data_bm25,column_data_model,version))\n",
        "\n",
        "print(f'BM25L preprocessed {recalls_processed} --> Média: {round(np.average(recalls_processed)*100,2)}. Desvio Padrão: {np.std(recalls_processed)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIxICUXEbFia",
        "outputId": "d80343b4-51b3-467e-b3c6-a91500edc47d"
      },
      "id": "lIxICUXEbFia",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [1:42:09<00:00, 1225.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25L preprocessed [0.6888888888888889, 0.8648648648648649, 0.8656716417910447, 0.9622641509433962, 0.8571428571428571] --> Média: 84.78. Desvio Padrão: 0.08837722742970304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VG7jT7SWrvhU",
        "gUIdaC6prgcU"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "f19f93563dacda22739fc5a3e009b744cceb1cf4fdf5c4ccdbebf018b7065255"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}