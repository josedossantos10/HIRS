{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josedossantos10/HIRS/blob/main/Hibrido_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning e avaliação do modelos"
      ],
      "metadata": {
        "id": "syCjnrYFMtOb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEypFq1Akp2U"
      },
      "source": [
        "Ajuste os parâmentros baixo e depois execute todo o codigo para avaliar o modelo hibrido."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coluna a ser usada para o Fini-Tuning e avaliação dos modelos\n",
        "# column_data = 'txtEmenta'\n",
        "# column_data = 'txtIndexacao'\n",
        "# column_data = 'hibrido'\n",
        "column_data = 'txtInteiroTeorLimpo'\n",
        "\n",
        "# Modelos de sentecnça para realizar o Fine-Tuning: 'lbert', 'labse', 'lbt', 'raq', veja a função build_model() para consultar os nomes de cada modelo diponível\n",
        "version = 'bertb'\n",
        "\n",
        "# Número de documentos a serem recuperados para avaliar os modelos\n",
        "top_k = 20\n",
        "\n",
        "# Número de épocas para realizar o Fine-Tuning\n",
        "epochs = 1\n",
        "\n",
        "# Número máximo de tokens para cada documento\n",
        "max_tokens = 512\n"
      ],
      "metadata": {
        "id": "5dWjeiW3LqMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação"
      ],
      "metadata": {
        "id": "efBcYXwWfqSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cWbP0YMffuDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f206b2c-cf90-4547-ead9-3a99bd088449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwh1aN-NKxar",
        "outputId": "03952a8a-af0a-460b-bcf2-d2b5afe7fab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m163.8/171.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, SentencesDataset, util, SentencesDataset, InputExample, losses, models\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import requests\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import os\n",
        "import re\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jvBNO_nmnuS"
      },
      "outputs": [],
      "source": [
        "os.makedirs('models',exist_ok=True)\n",
        "os.makedirs('data',exist_ok=True)\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V8_QLZxytGC"
      },
      "outputs": [],
      "source": [
        "def get_file(url, folder='data'):\n",
        "  r = requests.get(url.split('?').pop(0)+'?download=1')\n",
        "  d = r.headers['content-disposition']\n",
        "  fname = re.findall(\"filename=(.+)\", d)[0].replace('\"','')\n",
        "  with open(f'/content/{folder}/{fname}','wb') as f:\n",
        "    f.write(r.content)\n",
        "  if '.zip' in fname:\n",
        "    !unzip -o '/content/{folder}/{fname}' -d '/content/{folder}/'\n",
        "    print('Unziped and',end=' ')\n",
        "  print('Saved '+fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u0IAhkdwun1"
      },
      "outputs": [],
      "source": [
        "def build_model(version, max_tokens = 512):\n",
        "      if version=='bertb':\n",
        "          word_embedding_model = models.Transformer(\"neuralmind/bert-large-portuguese-cased\")\n",
        "          pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                                      pooling_mode_mean_tokens=True,\n",
        "                                      pooling_mode_cls_token=False,\n",
        "                                      pooling_mode_max_tokens=False)\n",
        "      elif version=='lbert':\n",
        "          word_embedding_model = SentenceTransformer(\"ulysses-camara/legal-bert-pt-br\")[0]\n",
        "          pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                                      pooling_mode_mean_tokens=True,\n",
        "                                      pooling_mode_cls_token=False,\n",
        "                                      pooling_mode_max_tokens=False)\n",
        "      elif version=='labse':\n",
        "          word_embedding_model = models.Transformer(\"sentence-transformers/LabSE\")\n",
        "          pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                                      pooling_mode_mean_tokens=False,\n",
        "                                      pooling_mode_cls_token=True,\n",
        "                                      pooling_mode_max_tokens=False)\n",
        "      elif version=='lbt':\n",
        "          word_embedding_model = models.Transformer(\"rufimelo/Legal-BERTimbau-large\")\n",
        "          pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                                      pooling_mode_mean_tokens=True,\n",
        "                                      pooling_mode_cls_token=False,\n",
        "                                      pooling_mode_max_tokens=False)\n",
        "      elif version=='raq':\n",
        "          word_embedding_model = models.Transformer(\"raquelsilveira/legalbertpt_fp\")\n",
        "          pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                                      pooling_mode_mean_tokens=True,\n",
        "                                      pooling_mode_cls_token=False,\n",
        "                                      pooling_mode_max_tokens=False)\n",
        "      else:\n",
        "          return SentenceTransformer(f'./models/{version}/')\n",
        "\n",
        "      word_embedding_model.max_seq_length=max_tokens\n",
        "      model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "      return model\n",
        "\n",
        "def finetuning(version, train_dataset, max_tokens, epochs=1, batch_size=2):\n",
        "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "    # Building dataloader and trianing model\n",
        "    model = build_model(version, max_tokens, True)\n",
        "    train_loss = losses.ContrastiveLoss(model)\n",
        "    #Tune the model\n",
        "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=epochs,show_progress_bar=True)\n",
        "    return model\n",
        "\n",
        "def get_name(df, doc, in_field=\"content\", out_field = \"name\"):\n",
        "    return str(df[df[in_field]==doc][out_field].to_numpy()[0]).strip()\n",
        "def recall(vector):\n",
        "    return sum([1 for (target, docs) in vector if target in docs])/len(vector)\n",
        "def evaluate(model, top_k,corpus, queries):\n",
        "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "    res_vector = list()\n",
        "    for query in queries:\n",
        "        try:\n",
        "            query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "            cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "            top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                top_resuts_txt = [get_name(df, txt, in_field=column_data, out_field=\"txtNome\").strip() for txt in corpus[top_results[1].cpu().numpy()]]\n",
        "            else:\n",
        "                top_resuts_txt = [get_name(df, txt, in_field=column_data, out_field=\"txtNome\").strip() for txt in corpus[top_results[1].numpy()]]\n",
        "\n",
        "            res_vector.append([get_name(df_assunto, query, \"TxtAssunto\", \"NÚMERO-PROPOSIÇÃOSILEG\").strip(), top_resuts_txt])\n",
        "        except Exception as e:\n",
        "            print(f\"ERRO ao realizar encoding:{e}\")\n",
        "            pass\n",
        "    return recall(res_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtenção dos dados"
      ],
      "metadata": {
        "id": "BKjnJWAjB9iN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CneX50PiDoPK",
        "outputId": "29535320-aef4-4f1f-8c37-41f247c2e572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/base_20230428_douglas-bill_corpus-parts.zip\n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part0.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part1.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part2.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part3.csv  \n",
            "  inflating: /content/data/base_20230428_douglas-bill_corpus-part4.csv  \n",
            "Unziped and Saved base_20230428_douglas-bill_corpus-parts.zip\n",
            "Saved pares-arvores-proposicao-filter.csv\n",
            "Archive:  /content/data/dados-conle-anonimizado-job_request-parts.zip\n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part0.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part1.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part2.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part3.csv  \n",
            "  inflating: /content/data/dados-conle-anonimizado-job_request-part4.csv  \n",
            "Unziped and Saved dados-conle-anonimizado-job_request-parts.zip\n"
          ]
        }
      ],
      "source": [
        "get_file('https://ufrpebr-my.sharepoint.com/:u:/g/personal/joseantonio_santos_ufrpe_br/EQcLlSaOKPtCtzY3juKPkpYB6Rl9Jnp-xNimWaIcJNe_bg?e=jc0ZWR')\n",
        "get_file('https://ufrpebr-my.sharepoint.com/:x:/g/personal/joseantonio_santos_ufrpe_br/EfciCkFDXkxAoKnvwW-O0FYB1YMf-iXIwD1nVdf0Ve_t8g?e=X1QPUi')\n",
        "get_file('https://ufrpebr-my.sharepoint.com/:u:/g/personal/joseantonio_santos_ufrpe_br/ESigtraTg-xLjUAwHuEzF0sB-CkOhU75tNSZcomdbuvpxg?e=AcMaXi')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento e Avaliação"
      ],
      "metadata": {
        "id": "zecF5DqxCgqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "zero_shot = []\n",
        "tuned = []\n",
        "\n",
        "read_df = pd.read_csv(f'/content/data/pares-arvores-proposicao-filter.csv').rename(columns={'imgArquivoTeorPDF_clean_1':'txtInteiroTeorLimpo_1','imgArquivoTeorPDF_clean_2':'txtInteiroTeorLimpo_2'}).dropna().reset_index(drop=True)\n",
        "read_df['label'] = read_df.label.apply(int)\n",
        "train_dataset = read_df.apply(lambda x: InputExample(texts=(x[f'{column_data}_1'], x[f'{column_data}_2']), label=x['label']), axis=1)\n",
        "\n",
        "model_tuned = finetuning(version, train_dataset, max_tokens,epochs=epochs, batch_size=2)\n",
        "for e in tqdm(range(0, runs)):\n",
        "    element = f'{column_data}_{version}_{e}'\n",
        "    df = pd.read_csv(f\"/content/data/base_20230428_douglas-bill_corpus-part{i}.csv\")\n",
        "    df_assunto = pd.read_csv(f\"/content/data/dados-conle-anonimizado-job_request-part{i}.csv\", encoding=\"utf-8\")\n",
        "    queries = df_assunto[\"TxtAssunto\"].to_numpy()\n",
        "    corpus = df[column_data].to_numpy()\n",
        "    zero_shot.append(evaluate(build_model(version, max_tokens),top_k,corpus, queries))\n",
        "    tuned.append(evaluate(model_tuned,top_k,corpus, queries))\n",
        "print(f'Zero shot {zero_shot} --> Média: {round(np.average(zero_shot)*100,2)}. Desvio Padrão: {np.std(zero_shot)}')\n",
        "print(f'Zero shot {tuned} --> Média: {round(np.average(tuned)*100,2)}. Desvio Padrão: {np.std(tuned)}')\n"
      ],
      "metadata": {
        "id": "F3AgBM3qaX8d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}